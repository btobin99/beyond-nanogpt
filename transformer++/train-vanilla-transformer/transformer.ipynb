{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m mhsa \u001b[38;5;241m=\u001b[39m Attention(D, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mmhsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# expect [B, S, D] ie. [1, 512, 768]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m B, S, D \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# let's make this multi-head now, ie. make each QKV [B, S, D] --> [B, nh, S, hd]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m Q, K, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWk(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWv(x) \u001b[38;5;66;03m# all [B, S, D]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m Q \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mview(B, S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# [B, nh, S, hd]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mview(B, S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "class Attention(torch.nn.Module): # BSD -> BSD\n",
    "    def __init__(self, D=768, head_dim=64, causal=True, device=\"cuda\"): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.head_dim = head_dim\n",
    "        assert D % head_dim == 0\n",
    "        self.nheads = D//head_dim\n",
    "        self.Wq = torch.nn.Linear(D, D)\n",
    "        self.Wk = torch.nn.Linear(D, D)\n",
    "        self.Wv = torch.nn.Linear(D, D)\n",
    "        self.causal = causal \n",
    "        self.Wo = torch.nn.Linear(D, D)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x): # input is [B, S, D] \n",
    "        B, S, D = x.shape\n",
    "        # let's make this multi-head now, ie. make each QKV [B, S, D] --> [B, nh, S, hd]\n",
    "\n",
    "        Q, K, V = self.Wq(x), self.Wk(x), self.Wv(x) # all [B, S, D]\n",
    "\n",
    "        Q = Q.view(B, S, self.nheads, self.head_dim).transpose(1,2) # [B, nh, S, hd]\n",
    "        K = K.view(B, S, self.nheads, self.head_dim).transpose(1,2)\n",
    "        V = V.view(B, S, self.nheads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # [B, nh, S, hd] @ [B, nh, hd, S] -> [B, nh, S, S]\n",
    "        logits = (Q@K.transpose(-2, -1))/torch.sqrt(torch.tensor(self.head_dim, device=self.device)) # [B, nh, S, S]\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(logits), diagonal=1).bool()\n",
    "            logits_masked = logits.masked_fill(mask, float('-inf'))\n",
    "        else:\n",
    "            logits_masked = logits\n",
    "\n",
    "        A = torch.nn.functional.softmax(logits_masked, dim=-1) # [B, nh, S, S]\n",
    "        \n",
    "        preout = torch.einsum('bnxy,bnyd->bnxd', A, V) # [B, nh, S, S] @ [B, nh, S, hd] -> [B, nh, S, hd]\n",
    "        preout = preout.transpose(1, 2).reshape(B, S, -1) # [B, nh, S, hd] -> [B, S, nh * hd]\n",
    "        \n",
    "        out = self.Wo(preout) # [B, S, D]\n",
    "        return out # [B, S, D]\n",
    "\n",
    "B, S, D = 1, 512, 768\n",
    "x = torch.randn(B, S, D)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "mhsa = Attention(D, device=device)\n",
    "mhsa(x).shape # expect [B, S, D] ie. [1, 512, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACT2FN = {\n",
    "    'relu': torch.nn.functional.relu,\n",
    "    'gelu': torch.nn.functional.gelu,\n",
    "    'silu': torch.nn.functional.silu,\n",
    "    'swish': torch.nn.functional.silu,\n",
    "}\n",
    "\n",
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self, D, hidden_multiplier=4, act='swish', device=None): \n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.up_proj = torch.nn.Linear(D, D*hidden_multiplier).to(self.device)\n",
    "        self.down_proj = torch.nn.Linear(D*hidden_multiplier, D).to(self.device)\n",
    "        self.act = ACT2FN[act]\n",
    "\n",
    "    def forward(self, x): # BSD -> BSD automatically on last dim \n",
    "        x = x.to(self.device)\n",
    "        return self.down_proj(self.act(self.up_proj(x)))\n",
    "\n",
    "B, S, D = 1, 512, 768\n",
    "x = torch.randn(B, S, D)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "mlp = MLP(D, device=device)\n",
    "mlp(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LN(torch.nn.Module): \n",
    "    def __init__(self, D, eps=1e-9, device=None): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.eps = eps\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mean_scale = torch.nn.Parameter(torch.zeros(D).to(self.device))\n",
    "        self.std_scale = torch.nn.Parameter(torch.ones(D).to(self.device))\n",
    "\n",
    "    def forward(self, x): # x is [B, S, D]\n",
    "        x = x.to(self.device)\n",
    "        mean = x.mean(dim=-1, keepdim=True) # [B, S, 1]\n",
    "        std = (x.var(dim=-1, keepdim=True) + self.eps)**0.5 # [B, S, 1]\n",
    "        x_norm = (x - mean)/(std) \n",
    "        return x_norm * self.std_scale + self.mean_scale\n",
    "\n",
    "B, S, D = 1, 512, 768\n",
    "x = torch.randn(B, S, D)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "ln = LN(D, device=device)\n",
    "ln(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(torch.nn.Module): \n",
    "    def __init__(self, D, device=None): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attn = Attention(D, device=self.device)\n",
    "        self.mlp = MLP(D, device=self.device)\n",
    "        self.ln1 = LN(D, device=self.device)\n",
    "        self.ln2 = LN(D, device=self.device)  \n",
    "    \n",
    "    def forward(self, x): # x is BSD\n",
    "        x = x.to(self.device)\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x \n",
    "\n",
    "\n",
    "class EmbeddingLayer(torch.nn.Module): \n",
    "    # this is just a lookup table \n",
    "    def __init__(self, vocab_size, D, device=None): \n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding = torch.nn.Parameter(torch.randn(vocab_size, D).to(self.device))\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x.to(self.device)\n",
    "        return self.embedding[x]\n",
    "\n",
    "class UnembeddingLayer(torch.nn.Module): \n",
    "    # this is just a lookup table that maps embeddings back to logits\n",
    "    def __init__(self, vocab_size, D, device=None): \n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.unembedding = torch.nn.Linear(D, vocab_size).to(self.device)\n",
    "\n",
    "    def forward(self, x): # x is [B, S, D]\n",
    "        # Return logits of shape [B, S, vocab_size]\n",
    "        x = x.to(self.device)\n",
    "        return self.unembedding(x)\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module): \n",
    "    def __init__(self, depth, hidden_dim, vocab_size, device=None): \n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.D = hidden_dim \n",
    "        self.depth = depth \n",
    "        self.emb = EmbeddingLayer(vocab_size, hidden_dim, device=self.device)\n",
    "        self.unemb = UnembeddingLayer(vocab_size, hidden_dim, device=self.device)\n",
    "        self.layers = torch.nn.ModuleList([TransformerLayer(self.D, device=self.device) for _ in range(depth)])\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x): # x is tokenized inputs, so BS1\n",
    "        x = x.to(self.device)\n",
    "        x = self.emb(x) # BSD \n",
    "        for layer in self.layers:\n",
    "            x = layer(x) # BSD \n",
    "        x = self.unemb(x) # BSV \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "Available splits: dict_keys(['train', 'validation'])\n",
      "Number of examples in train: 2119719\n",
      "Number of examples in validation: 21990\n",
      "\n",
      "Sample story:\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "# Download TinyStories dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"Available splits: {dataset.keys()}\")\n",
    "print(f\"Number of examples in train: {len(dataset['train'])}\")\n",
    "print(f\"Number of examples in validation: {len(dataset['validation'])}\")\n",
    "\n",
    "# Display a sample story\n",
    "sample_story = dataset['train'][0]\n",
    "print(\"\\nSample story:\")\n",
    "print(sample_story['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                  | 9121/2119719 [00:00<00:23, 91203.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2119719/2119719 [00:23<00:00, 88533.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "tokens = set()\n",
    "for example in tqdm(dataset['train']):\n",
    "    tokens.update(example['text'])\n",
    "tokens = list(tokens)\n",
    "tokenizer = {tok:i for (i,tok) in enumerate(tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(el): \n",
    "    return list(map(lambda x: tokenizer[x], el))\n",
    "\n",
    "sentence = dataset['train'][0]['text']\n",
    "tokenize(sentence)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, seq_len, device='cuda'):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.current_idx = 0\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current_idx = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch_texts = []\n",
    "        batch_tokens = []\n",
    "        \n",
    "        # Collect batch_size examples\n",
    "        for i in range(self.batch_size):\n",
    "            if self.current_idx >= len(self.dataset):\n",
    "                break\n",
    "                \n",
    "            text = self.dataset[self.current_idx]['text']\n",
    "            tokens = tokenize(text)\n",
    "            \n",
    "            # Truncate or pad to seq_len\n",
    "            if len(tokens) > self.seq_len:\n",
    "                tokens = tokens[:self.seq_len]\n",
    "            elif len(tokens) < self.seq_len:\n",
    "                tokens = tokens + [0] * (self.seq_len - len(tokens))\n",
    "                \n",
    "            batch_texts.append(text)\n",
    "            batch_tokens.append(tokens)\n",
    "            self.current_idx += 1\n",
    "        \n",
    "        # Convert to tensors and move to CUDA\n",
    "        batch_tokens = torch.tensor(batch_tokens, device=self.device)\n",
    "        return batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, D, nlayers, vocab_size = 16, 512, 768, 8, len(tokenizer.items())\n",
    "boi = Transformer(nlayers, D, vocab_size)\n",
    "dataloader = DataLoader(dataset['train'], B, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 174])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(dataloader)\n",
    "boi(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_356795/3743115951.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inference_model.load_state_dict(torch.load(save_path, map_location='cuda'))\n",
      "  0%|                                                                                                                                                                 | 0/2119719 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2119719/2119719 [00:23<00:00, 91110.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 317.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeE41 E41 E4d®E41d* ˜‰d®déTE{I{IE*1 Ey¹D–„­yEÊ® EyEéEdZ eEQIé EGÊyEKdy1EyIEy1 EÊTEyEÊ *E{kIE{Ê d®E*EyE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize(tokenizer, el): \n",
    "    return list(map(lambda x: tokenizer[x], el))\n",
    "\n",
    "# Define model parameters (should match those used during training)\n",
    "# For example, when training with: python3 train.py --hidden_dim 768 --nlayers 6 --verbose --steps 250 --save\n",
    "hidden_dim = 512  # Set to 768 as specified during training\n",
    "nlayers = 6\n",
    "vocab_size = len(tokenizer.items())\n",
    "\n",
    "# Initialize a new model with the same architecture\n",
    "inference_model = Transformer(nlayers, hidden_dim, vocab_size, device='cuda')\n",
    "\n",
    "# Load the saved weights with a proper device mapping\n",
    "save_dir = \"/n/netscratch/gershman_lab/Lab/tkumar/datasets/dclm/global-shard_01_of_10/newest_data/gravity-models/\"\n",
    "model_name = \"model_lr0.0003_bs256_seq512.pt\"  # Matches training parameters\n",
    "save_path = f\"{save_dir}/{model_name}\"\n",
    "inference_model.load_state_dict(torch.load(save_path, map_location='cuda'))\n",
    "inference_model.eval()  # Set to evaluation mode\n",
    "\n",
    "def build_tokenizer_for_dataset(dataset):\n",
    "    tokens = set()\n",
    "    # Process a subset of the dataset to build the tokenizer\n",
    "    for example in tqdm(dataset['train']):\n",
    "        tokens.update(example['text'])\n",
    "    tokens = sorted(list(tokens))  # Sort to ensure consistent ordering\n",
    "    tokenizer = {tok: i for (i, tok) in enumerate(tokens)}\n",
    "    return tokenizer\n",
    "\n",
    "# Rebuild the tokenizer\n",
    "tokenizer = build_tokenizer_for_dataset(dataset)\n",
    "\n",
    "# Now fix the generate_text function\n",
    "def generate_text(model, tokenizer, prompt=\"Once upon a time\", max_length=100, temperature=1.0):\n",
    "    # Create reverse mapping\n",
    "    reverse_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "    \n",
    "    # Convert the prompt to tokens character by character\n",
    "    tokens = []\n",
    "    for char in prompt:\n",
    "        if char in tokenizer:\n",
    "            tokens.append(tokenizer[char])\n",
    "        else:\n",
    "            print(f\"Warning: Character '{char}' not in tokenizer\")\n",
    "    \n",
    "    input_tokens = torch.tensor([tokens], device=model.device)\n",
    "    \n",
    "    # Generate tokens one at a time\n",
    "    for _ in tqdm(range(max_length)):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tokens)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply a mask to only select valid token indices\n",
    "            # This prevents selecting tokens that don't exist in the reverse_tokenizer\n",
    "            valid_indices = torch.tensor([i for i in range(len(logits[0, -1, :])) \n",
    "                                         if i in reverse_tokenizer], \n",
    "                                        device=model.device)\n",
    "            \n",
    "            if len(valid_indices) == 0:\n",
    "                break\n",
    "                \n",
    "            masked_logits = torch.ones_like(next_token_logits) * float('-inf')\n",
    "            masked_logits[valid_indices] = next_token_logits[valid_indices]\n",
    "            \n",
    "            probabilities = torch.nn.functional.softmax(masked_logits, dim=0)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            input_tokens = torch.cat([input_tokens, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Convert the generated tokens back to text\n",
    "    generated_tokens = input_tokens[0].tolist()\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    for token in generated_tokens:\n",
    "        if token in reverse_tokenizer:\n",
    "            generated_text += reverse_tokenizer[token]\n",
    "        else:\n",
    "            generated_text += \"<?>\"  # Placeholder for unknown tokens\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Try generating text again\n",
    "sample_text = generate_text(inference_model, tokenizer, prompt=\"Once upon a time\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
